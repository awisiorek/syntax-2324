{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax natürlicher Sprachen, WS 2023/24\n",
    "\n",
    "# 12 - Übung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import nltk\n",
    "from nltk import conlltags2tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1 - IOB-Tagging\n",
    "\n",
    "### a) Vervollständigen Sie den die folgende *IOB-Tag-Sequenz* einer NP-VP-Chunk-Analyse erfüllenden deutschen Satz.\n",
    "\n",
    "#### (Beachten Sie auch die gegebenen Satzzeichen bzgl. möglicher Satztypen und entsprechender Worstellung.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iob_list = [\n",
    "(\"B-NP\", \"\"),\n",
    "(\"B-VP\", \"\"),\n",
    "(\"O\", \"\"),\n",
    "(\"O\", \",\"),\n",
    "(\"O\", \"\"),\n",
    "(\"B-NP\", \"\"),\n",
    "(\"B-NP\", \"\"),\n",
    "(\"I-NP\", \"\"),\n",
    "(\"O\", \"\"),\n",
    "(\"B-VP\", \"\"),\n",
    "(\"O\", \".\")\n",
    "]\n",
    "\n",
    "tree = conlltags2tree([(word, \"\", iobtag)for iobtag, word in iob_list])\n",
    "tree.pretty_print(unicodelines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Geben Sie die *IOB-Tag-Sequenz* und *POS-Label* einer vollständigen flachen Chunk-Analyse für den folgenden Satz an:\n",
    "\n",
    "- *Während sich Kafka in Briefen, Tagebüchern und Prosatexten umfangreich mit seinem Verhältnis zum Vater auseinandersetzte, stand die Beziehung zu seiner Mutter eher im Hintergrund.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         S            \n",
      "         │             \n",
      "         NP           \n",
      "   ┌─────┴──────┐      \n",
      "ein/DET     Beispiel/N\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iob_list = [\n",
    "(\"ein\", \"DET\", \"B-NP\"),\n",
    "(\"Beispiel\", \"N\", \"I-NP\"),\n",
    "]\n",
    "\n",
    "tree = conlltags2tree(iob_list)\n",
    "tree.pretty_print(unicodelines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iob_list = [\n",
    "(\"Während\", \"\", \"\"),\n",
    "(\"sich\", \"\", \"\"),\n",
    "(\"Kafka\", \"\", \"\"),\n",
    "(\"in\", \"\", \"\"),\n",
    "(\"Briefen,\", \"\", \"\"),\n",
    "(\"Tagebüchern\", \"\", \"\"),\n",
    "(\"und\", \"\", \"\"),\n",
    "(\"Prosatexten\", \"\", \"\"),\n",
    "(\"umfangreich\", \"\", \"\"),\n",
    "(\"mit\", \"\", \"\"),\n",
    "(\"seinem\", \"\", \"\"),\n",
    "(\"schwierigen\", \"\", \"\"),\n",
    "(\"Verhältnis\", \"\", \"\"),\n",
    "(\"zum\", \"\", \"\"),\n",
    "(\"Vater\", \"\", \"\"),\n",
    "(\"auseinandersetzte\", \"\", \"\"),\n",
    "(\",\", \"PUNCT\", \"O\"),\n",
    "(\"stand\", \"\", \"\"),\n",
    "(\"die\", \"\", \"\"),\n",
    "(\"Beziehung\", \"\", \"\"),\n",
    "(\"zu\", \"\", \"\"),\n",
    "(\"seiner\", \"\", \"\"),\n",
    "(\"Mutter\", \"\", \"\"),\n",
    "(\"eher\", \"\", \"\"),\n",
    "(\"im\", \"\", \"\"),\n",
    "(\"Hintergrund\", \"\", \"\"),\n",
    "(\".\", \"PUNCT\", \"O\")\n",
    "]\n",
    "\n",
    "tree = conlltags2tree(iob_list)\n",
    "tree.pretty_print(unicodelines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Aufgabe 2 - Evaluationsmetriken*\n",
    "\n",
    "#### Betrachten Sie folgende Daten. Es handelt sich um ein vereinfachtes Tagging-Schema fürs Chunking, bei dem nur zwischen „Teil einer NP“ (`1`) und „nicht Teil einer NP“ (`0`) unterschieden wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = [1,0,1,0,0,1,1,1,1,0]\n",
    "chunker1     = [1,1,1,0,1,0,1,1,1,1]\n",
    "chunker2     = [1,0,1,0,0,0,0,0,1,0]\n",
    "chunker3     = [0,0,0,0,0,1,1,1,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Berechnen Sie für jeden der Chunker Accuracy, Precision, Recall und F1-Score zunächst per Hand und überprüfen Sie dann Ihr Ergebnis mit dem folgenden Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(chunker):\n",
    "    print(\n",
    "        \"Accuracy:\",\n",
    "        \"{:.2f}\".format(accuracy_score(ground_truth, chunker))\n",
    "    )\n",
    "    print(\n",
    "        \"Precision:\",\n",
    "        \"{:.2f}\".format(precision_score(ground_truth, chunker))\n",
    "    )\n",
    "    print(\n",
    "        \"Recall:\",\n",
    "        \"{:.2f}\".format(recall_score(ground_truth, chunker))\n",
    "    )\n",
    "    print(\n",
    "        \"F1-Score:\",\n",
    "        \"{:.2f}\".format(f1_score(ground_truth, chunker))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunker 1\n",
    "\n",
    "#Accuracy: \n",
    "#Precision: \n",
    "#Recall: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(chunker1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunker 2\n",
    "\n",
    "#Accuracy: \n",
    "#Precision: \n",
    "#Recall: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(chunker2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunker 3\n",
    "\n",
    "#Accuracy: \n",
    "#Precision: \n",
    "#Recall: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(chunker3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Aufgabe 3 - Herunterladen von Ressourcen*\n",
    "\n",
    "#### Das CoNLL 2000 Korpus ist ein POS- und Chunk-getaggtes Korpus (IOB- Format), das in ein Test- und ein Trainingskorpus aufgeteilt ist. Wir werden es zum Training und zur Evaluation von Chunk-Parsern verwenden. Laden Sie es sich dafür zunächst über die Ressource `corpora/conll2000` herunter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wenn Sie es erfolgreich heruntergeladen haben, können Sie folgendermaßen darauf zugreifen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,512.0,168.0\" width=\"512px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"9.375%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Over</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"4.6875%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"14.0625%\" x=\"9.375%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"44.4444%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">a</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.2222%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"55.5556%\" x=\"44.4444%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cup</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.2222%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.4062%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.25%\" x=\"23.4375%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">of</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.5625%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"12.5%\" x=\"29.6875%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">coffee</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"35.9375%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.6875%\" x=\"42.1875%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"44.5312%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"18.75%\" x=\"46.875%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"41.6667%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Mr.</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20.8333%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"58.3333%\" x=\"41.6667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Stone</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70.8333%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"56.25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"9.375%\" x=\"65.625%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">told</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70.3125%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"20.3125%\" x=\"75%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"46.1538%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">his</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">PRP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.0769%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"53.8462%\" x=\"46.1538%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">story</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"73.0769%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.1562%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.6875%\" x=\"95.3125%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"97.6562%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [('Over', 'IN'), Tree('NP', [('a', 'DT'), ('cup', 'NN')]), ('of', 'IN'), Tree('NP', [('coffee', 'NN')]), (',', ','), Tree('NP', [('Mr.', 'NNP'), ('Stone', 'NNP')]), ('told', 'VBD'), Tree('NP', [('his', 'PRP$'), ('story', 'NN')]), ('.', '.')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Das `chunk_types`-Argument dient der Auwahl von Chunk-Typen (in diesem Beispiel Nominalphrasen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Aufgabe 4 - Chunking mit regulären Ausdrücken*\n",
    "\n",
    "#### Erstellen Sie einen einfachen `RegexpParser`, der für Nominalphrasen charakteristische Tags zu NPs zusammenfasst. Solche charakteristischen Tags sind z.B. Kardinalzahlen (`CD`), Artikel (`DT`), Adjektive (`JJ`, `JJR`, `JJS`) und natürlich Substantive (`NN`, `NNS`, `NNP`, `NNPS`).\n",
    "\n",
    "#### Weitere interessante Tags wären `PDT` (z.B. *both*, *a lot of*), `POS` (*'s*), `PRP` (Personalpronomen), `PRP$`(Possessivpronomen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r\"\"\"NP:\n",
    "{<>}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluieren Sie Ihren Parser anschließend auf dem CoNLL 2000 Korpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  43.4%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n"
     ]
    }
   ],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "cp = nltk.RegexpParser(regex)\n",
    "print(cp.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Aufgabe 5 - Datenbasiertes Chunking*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Betrachten Sie den folgenden Code für einen Chunker, der für jedes POS-Tag das wahrscheinlichste Chunk-Tag berechnet (Training) und dieses dann zur Testzeit ausgibt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [\n",
    "            [\n",
    "                (t,c)\n",
    "                for w,t,c in nltk.chunk.tree2conlltags(sent)\n",
    "            ]\n",
    "            for sent in train_sents\n",
    "        ]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [\n",
    "            chunktag for (pos, chunktag) in tagged_pos_tags\n",
    "        ]\n",
    "        conlltags = [\n",
    "            (word, pos, chunktag)\n",
    "            for ((word, pos), chunktag)\n",
    "            in zip(sentence, chunktags)\n",
    "        ]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainieren und evaluieren Sie den UnigramChunker auf dem CoNLL 2000 Korpus für NPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "uc = UnigramChunker(train_sents)\n",
    "print(uc.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Um uns einen Überblick darüber zu verschaffen, was der Chunker gelernt hat, können wir ihn für jedes mögliche POS-Tag eine Vorhersage treffen lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postags = sorted(set(pos for sent in train_sents for (word,pos) in sent.leaves()))\n",
    "uc.tagger.tag(postags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) der `ConsecutiveNPChunker`, dessen Code Sie in der nächsten Zelle sehen, basiert auf einem Klassifikator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
    "\n",
    "    def __init__(self, train_sents, npchunk_features):\n",
    "        self.extract_features = npchunk_features\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = self.extract_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents, npchunk_features):\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "                         nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents, npchunk_features)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dies erlaubt uns, die Features, die für die Klassifikation extrahiert werden, genauer zu bestimmen.\n",
    "\n",
    "#### Ein Feature-Extraktor lässt sich als Funktion z.B. so definieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_feature(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    return {\"pos\": pos}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dieser Feature-Extraktor extrahiert genau ein Feature, nämlich das POS-Tag, das auch der UnigramChunker verwendet hat.\n",
    "\n",
    "#### Evaluieren Sie den `ConsecutiveNPChunker` mit diesem Feature-Extraktor und vergleichen Sie seine Performanz mit der des `UnigramChunker`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = ConsecutiveNPChunker(train_sents, pos_feature)\n",
    "print(chunker.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Fügen Sie weitere Features für:\n",
    "- das aktuelle Wort\n",
    "- das vorhergehende POS-Tag\n",
    "- das vorhergehende Chunk-Tag\n",
    "\n",
    "#### zur Extraktion hinzu und beobachten Sie jeweils die Auswirkungen auf die Performanz in der Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feature(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    return {\"pos\": pos}\n",
    "\n",
    "def previous_pos(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    return {\"pos\": pos}\n",
    "\n",
    "def previous_chunk(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    return {\"pos\": pos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = ConsecutiveNPChunker(train_sents, word_feature)\n",
    "print(chunker.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = ConsecutiveNPChunker(train_sents, previous_pos)\n",
    "print(chunker.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = ConsecutiveNPChunker(train_sents, previous_chunk)\n",
    "print(chunker.accuracy(test_sents))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
